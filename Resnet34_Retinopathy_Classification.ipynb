{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB-aa1orEaW-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT_s0FZcimEU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn import Conv2d\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.transforms import Resize\n",
        "from torchvision.transforms import CenterCrop\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import pil_to_tensor\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx5Dh9O1fB7D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpmfQuwrk5iH",
        "outputId": "8cef2b0e-7332-4332-c69e-afbc74cf1f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "data = '/content/drive/MyDrive/UCSB SRA Project/TrainingData_Final' #change to file path\n",
        "print()\n",
        "dirs = os.listdir(data)\n",
        "dataset_old = []\n",
        "for dir in dirs:\n",
        "  for inner_dir in os.listdir(os.path.join(data, dir)):\n",
        "      dataset_old.append(os.path.join(data, dir, inner_dir))\n",
        "dataset = []\n",
        "curr = 0\n",
        "for i in dataset_old:\n",
        "  if i[59] != '5':\n",
        "    dataset.append(i)\n",
        "  elif curr < 400:\n",
        "    dataset.append(i)\n",
        "    curr += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBnU5deX5W-T"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_set = []\n",
        "test_set = []\n",
        "validation_set = []\n",
        "for i in range(0, 6):\n",
        "  path = f\"/content/drive/MyDrive/UCSB SRA Project/TrainingData_Final/{i}\" #change to file path\n",
        "  all_imgs_0 = os.listdir(path)\n",
        "  all_imgs_0 = [f\"{path}/{_path}\" for _path in all_imgs_0]\n",
        "  if i == 5:\n",
        "    all_imgs_0 = all_imgs_0[:400]\n",
        "  num_imgs_0 = len(all_imgs_0)\n",
        "  arr = np.arange(num_imgs_0)\n",
        "  np.random.shuffle(arr)\n",
        "  curr_tr = len(train_set)\n",
        "  curr_te = len(test_set)\n",
        "  curr_va = len(validation_set)\n",
        "  train_set += [all_imgs_0[idx] for idx in arr[0:int(0.8*num_imgs_0)]]\n",
        "  test_set += [all_imgs_0[idx]  for idx in arr[int(0.8*num_imgs_0):int(0.9*num_imgs_0)]]\n",
        "  validation_set += [all_imgs_0[idx]  for idx in arr[int(0.9*num_imgs_0):int(num_imgs_0)]]\n",
        "  print(i, 'train: ', len(train_set) - curr_tr)\n",
        "  print(i, 'test: ', len(test_set) - curr_te)\n",
        "  print(i, 'val: ', len(validation_set) - curr_va)\n",
        "  print('---')\n",
        "print('total:', len(train_set) + len(test_set) + len(validation_set))\n",
        "print('train:', len(train_set))\n",
        "print('test:', len(test_set))\n",
        "print('val:', len(validation_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDWjyKB755Gs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "labels = []\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_paths):\n",
        "        self.file_paths = file_paths\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),  # Resize the image to (256, 256)\n",
        "            transforms.ToTensor(),          # Convert the PIL image to a PyTorch tensor\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the tensor\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        image = Image.open(file_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "\n",
        "        return image, file_path[59]\n",
        "train_dataloader = DataLoader(CustomDataset(train_set), batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(CustomDataset(test_set), batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(CustomDataset(validation_set), batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzkBSXIZ1eRF",
        "outputId": "79d0ea73-2d50-4ecc-e512-a7cb22d93999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvZliuxgXN_8"
      },
      "outputs": [],
      "source": [
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm4h0nIN4qJE"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
        "                        nn.BatchNorm2d(out_channels),\n",
        "                        nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(\n",
        "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
        "                        nn.BatchNorm2d(out_channels))\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViGQ6zkF4zs8"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes = 5):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.ReLU())\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
        "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
        "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
        "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eja3vJGu7v3-"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork2(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes = 2):\n",
        "        super(NeuralNetwork2, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.ReLU())\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
        "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
        "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
        "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t45q8xKqc22"
      },
      "outputs": [],
      "source": [
        "def label_changer(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 3 or y[i] == 5:\n",
        "      y[i] = 0\n",
        "    elif y[i] != 4:\n",
        "      y[i] += 1\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0, 0, 0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  return y\n",
        "from re import I\n",
        "model = NeuralNetwork(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "  # Compute prediction and loss\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer(y)\n",
        "    y = y.cpu()\n",
        "    pred = model(X)\n",
        "    pred = pred.to(torch.float32)\n",
        "    y = y.to(torch.float32)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    accuracy = (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "    print(accuracy)\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer(y)\n",
        "      y = y.cpu()\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrM6e1o4d3xx"
      },
      "outputs": [],
      "source": [
        "# label 0: Mild Diabetic Nonproliferative Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 0:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "      print(pred.argmax(1))\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW5Sj8z_fOeL"
      },
      "outputs": [],
      "source": [
        "torch.save(model2.state_dict(),  \"binary0model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8Fl_65Vd88B"
      },
      "outputs": [],
      "source": [
        "# label 1: Moderate Diabetic Nonproliferative Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 1:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIhdpflLfIgI"
      },
      "outputs": [],
      "source": [
        "torch.save(model2.state_dict(),  \"binary1model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUewMTcscp0b"
      },
      "outputs": [],
      "source": [
        "# label 2: Severe Diabetic Nonproliferative Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 2:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw5FXRRCfF98"
      },
      "outputs": [],
      "source": [
        "torch.save(model2.state_dict(),  \"binary2model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLexeoXcLylg"
      },
      "outputs": [],
      "source": [
        "# label 3: Hypertensive  Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 3:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAwfR61Ee_5A"
      },
      "outputs": [],
      "source": [
        "torch.save(model2.state_dict(),  \"binary3model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_g5Kt4-d_xR"
      },
      "outputs": [],
      "source": [
        "# label 4: Severe Diabetic Nonproliferative Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 4:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u-ijRihBmjF"
      },
      "outputs": [],
      "source": [
        "torch.save(model2.state_dict(),  \"binary4model\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}