{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB-aa1orEaW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fdca6d-d81b-42d4-993c-85eb6e5dd588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT_s0FZcimEU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn import Conv2d\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.transforms import Resize\n",
        "from torchvision.transforms import CenterCrop\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import pil_to_tensor\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx5Dh9O1fB7D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpmfQuwrk5iH",
        "outputId": "8cef2b0e-7332-4332-c69e-afbc74cf1f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# SAANVI COMPUTER CODE\n",
        "data = '/content/drive/MyDrive/UCSB SRA Project/TrainingData_Final'\n",
        "print()\n",
        "dirs = os.listdir(data)\n",
        "dataset_old = []\n",
        "for dir in dirs:\n",
        "  for inner_dir in os.listdir(os.path.join(data, dir)):\n",
        "      dataset_old.append(os.path.join(data, dir, inner_dir))\n",
        "# print(dataset_old)\n",
        "dataset = []\n",
        "curr = 0\n",
        "for i in dataset_old:\n",
        "  if i[59] != '5':\n",
        "    dataset.append(i)\n",
        "  elif curr < 400:\n",
        "    dataset.append(i)\n",
        "    curr += 1\n",
        "#59nd index\n",
        "# print(len(dataset), curr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBnU5deX5W-T",
        "outputId": "c37aab2a-c43d-4219-96ed-e7b3e06c7935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 train:  439\n",
            "0 test:  55\n",
            "0 val:  55\n",
            "---\n",
            "1 train:  796\n",
            "1 test:  99\n",
            "1 val:  100\n",
            "---\n",
            "2 train:  128\n",
            "2 test:  16\n",
            "2 val:  17\n",
            "---\n",
            "3 train:  153\n",
            "3 test:  19\n",
            "3 val:  20\n",
            "---\n",
            "4 train:  28\n",
            "4 test:  3\n",
            "4 val:  4\n",
            "---\n",
            "5 train:  320\n",
            "5 test:  40\n",
            "5 val:  40\n",
            "---\n",
            "total: 2332\n",
            "train: 1864\n",
            "test: 232\n",
            "val: 236\n"
          ]
        }
      ],
      "source": [
        "# SAANVI COMPUTER CODE\n",
        "train_set = []\n",
        "test_set = []\n",
        "validation_set = []\n",
        "for i in range(0, 6):\n",
        "  path = f\"/content/drive/MyDrive/UCSB SRA Project/TrainingData_Final/{i}\"\n",
        "  all_imgs_0 = os.listdir(path)\n",
        "  # /content/drive/MyDrive/TrainingData_Final/0\n",
        "  all_imgs_0 = [f\"{path}/{_path}\" for _path in all_imgs_0]\n",
        "  # print(all_imgs_0)\n",
        "  if i == 5:\n",
        "    all_imgs_0 = all_imgs_0[:400]\n",
        "    # print(len(all_imgs_0))\n",
        "  num_imgs_0 = len(all_imgs_0)\n",
        "  arr = np.arange(num_imgs_0)\n",
        "  np.random.shuffle(arr)\n",
        "  # print(i, 'total:', num_imgs_0)\n",
        "  curr_tr = len(train_set)\n",
        "  curr_te = len(test_set)\n",
        "  curr_va = len(validation_set)\n",
        "  train_set += [all_imgs_0[idx] for idx in arr[0:int(0.8*num_imgs_0)]]\n",
        "  test_set += [all_imgs_0[idx]  for idx in arr[int(0.8*num_imgs_0):int(0.9*num_imgs_0)]]\n",
        "  validation_set += [all_imgs_0[idx]  for idx in arr[int(0.9*num_imgs_0):int(num_imgs_0)]]\n",
        "  print(i, 'train: ', len(train_set) - curr_tr)\n",
        "  print(i, 'test: ', len(test_set) - curr_te)\n",
        "  print(i, 'val: ', len(validation_set) - curr_va)\n",
        "  print('---')\n",
        "print('total:', len(train_set) + len(test_set) + len(validation_set))\n",
        "print('train:', len(train_set))\n",
        "print('test:', len(test_set))\n",
        "print('val:', len(validation_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDWjyKB755Gs"
      },
      "outputs": [],
      "source": [
        "# SAANVI COMPUTER CODE\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "labels = []\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_paths):\n",
        "        self.file_paths = file_paths\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),  # Resize the image to (256, 256)\n",
        "            transforms.ToTensor(),          # Convert the PIL image to a PyTorch tensor\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the tensor\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        image = Image.open(file_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "\n",
        "        return image, file_path[59]\n",
        "train_dataloader = DataLoader(CustomDataset(train_set), batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(CustomDataset(test_set), batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(CustomDataset(validation_set), batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzkBSXIZ1eRF",
        "outputId": "79d0ea73-2d50-4ecc-e512-a7cb22d93999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvZliuxgXN_8"
      },
      "outputs": [],
      "source": [
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm4h0nIN4qJE"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
        "                        nn.BatchNorm2d(out_channels),\n",
        "                        nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(\n",
        "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
        "                        nn.BatchNorm2d(out_channels))\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViGQ6zkF4zs8"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes = 5):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.ReLU())\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
        "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
        "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
        "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eja3vJGu7v3-"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork2(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes = 2):\n",
        "        super(NeuralNetwork2, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.ReLU())\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
        "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
        "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
        "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "8CpZzVrY1j1Q",
        "outputId": "651b1f43-9529-4d0a-e5dd-a291cbdd9fcd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'class NeuralNetwork(nn.Module):\\n  def __init__(self):\\n    super(NeuralNetwork, self).__init__()\\n    self.linear_relu_stack = nn.Sequential(\\n      nn.Conv2d(3, 4, kernel_size=3),\\n      nn.ReLU(),\\n      nn.MaxPool2d(kernel_size=2, stride = 2),\\n      nn.Conv2d(4, 8, kernel_size=3),\\n      nn.ReLU(),\\n      nn.MaxPool2d(kernel_size=2, stride = 2),\\n      nn.Conv2d(8, 16, kernel_size=3),\\n      nn.ReLU(),\\n      nn.MaxPool2d(kernel_size=2, stride = 2),\\n      nn.Flatten(),\\n      nn.Linear(14400, 5000),\\n      nn.Linear(5000, 5), #nn.Linear(10000, 6)\\n      nn.Softmax(dim=1)\\n    )\\n  def forward(self, x):\\n    logits = self.linear_relu_stack(x)\\n    return logits'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "      nn.Conv2d(3, 4, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "      nn.Conv2d(4, 8, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "      nn.Conv2d(8, 16, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(14400, 5000),\n",
        "      nn.Linear(5000, 5), #nn.Linear(10000, 6)\n",
        "      nn.Softmax(dim=1)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "u8CzVAQNoU-R",
        "outputId": "d3e893b3-257f-4624-b9a6-fb63a3cf631e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'class NeuralNetwork2(nn.Module):\\n  def __init__(self):\\n    super(NeuralNetwork2, self).__init__()\\n    self.linear_relu_stack = nn.Sequential(\\n      nn.Conv2d(3, 4, kernel_size=3),\\n      nn.ReLU(),\\n      nn.MaxPool2d(kernel_size=2, stride = 2),\\n      nn.Conv2d(4, 8, kernel_size=3),\\n      nn.ReLU(),\\n      nn.MaxPool2d(kernel_size=2, stride = 2),\\n      nn.Conv2d(8, 16, kernel_size=3),\\n      nn.ReLU(),\\n      nn.MaxPool2d(kernel_size=2, stride = 2),\\n      nn.Flatten(),\\n      nn.Linear(14400, 5000),\\n      nn.Linear(5000, 2), #nn.Linear(10000, 6)\\n      # nn.Sigmoid(),\\n      # nn.softmax\\n    )\\n  def forward(self, x):\\n    logits = self.linear_relu_stack(x)\\n    return logits'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''class NeuralNetwork2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork2, self).__init__()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "      nn.Conv2d(3, 4, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "      nn.Conv2d(4, 8, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "      nn.Conv2d(8, 16, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(14400, 5000),\n",
        "      nn.Linear(5000, 2), #nn.Linear(10000, 6)\n",
        "      # nn.Sigmoid(),\n",
        "      # nn.softmax\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "2t45q8xKqc22",
        "outputId": "d1d909d7-b6f8-4cbc-99d3-184e22950d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d4051a6cdf9d>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m   \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-d4051a6cdf9d>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;31m# Compute prediction and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e4de09bbc3fb>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2982\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2984\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2986\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def label_changer(y):\n",
        "  for i in range(len(y)):\n",
        "    #type(y)\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 3 or y[i] == 5:\n",
        "      y[i] = 0\n",
        "    elif y[i] != 4:\n",
        "      y[i] += 1\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0, 0, 0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  return y\n",
        "from re import I\n",
        "model = NeuralNetwork(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "batch_size = 64\n",
        "# epochs = 5\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "  # Compute prediction and loss\n",
        "    X = X.cpu()\n",
        "    #y.expand(-1, 4)\n",
        "    y = list(y)\n",
        "    y = label_changer(y)\n",
        "    y = y.cpu()\n",
        "    pred = model(X)\n",
        "    pred = pred.to(torch.float32)\n",
        "    y = y.to(torch.float32)\n",
        "    # print(y.shape)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    # print(y)\n",
        "    # print(pred)\n",
        "    # print(pred.argmax(1))\n",
        "    # print('pred', pred)\n",
        "    # print('y', y)\n",
        "    accuracy = (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "    print(accuracy)\n",
        "    # print('---')\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer(y)\n",
        "      y = y.cpu()\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrM6e1o4d3xx",
        "outputId": "218be57e-13c3-48f9-b947-7afe4e1eac7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.611116 [    0/ 1864]\n",
            "loss: 6.981918 [   64/ 1864]\n",
            "loss: 0.790580 [  128/ 1864]\n",
            "loss: 0.780735 [  192/ 1864]\n",
            "loss: 0.852717 [  256/ 1864]\n",
            "loss: 0.699635 [  320/ 1864]\n",
            "loss: 0.780472 [  384/ 1864]\n",
            "loss: 1.374881 [  448/ 1864]\n",
            "loss: 1.648653 [  512/ 1864]\n",
            "loss: 2.419097 [  576/ 1864]\n",
            "loss: 1.701950 [  640/ 1864]\n",
            "loss: 1.240710 [  704/ 1864]\n",
            "loss: 0.634931 [  768/ 1864]\n",
            "loss: 0.559811 [  832/ 1864]\n",
            "loss: 0.660068 [  896/ 1864]\n",
            "loss: 0.759758 [  960/ 1864]\n",
            "loss: 0.821238 [ 1024/ 1864]\n",
            "loss: 0.767732 [ 1088/ 1864]\n",
            "loss: 0.722568 [ 1152/ 1864]\n",
            "loss: 0.607328 [ 1216/ 1864]\n",
            "loss: 0.479482 [ 1280/ 1864]\n",
            "loss: 0.936873 [ 1344/ 1864]\n",
            "loss: 0.780490 [ 1408/ 1864]\n",
            "loss: 0.978710 [ 1472/ 1864]\n",
            "loss: 0.550032 [ 1536/ 1864]\n",
            "loss: 0.569752 [ 1600/ 1864]\n",
            "loss: 0.584130 [ 1664/ 1864]\n",
            "loss: 0.825615 [ 1728/ 1864]\n",
            "loss: 0.597856 [ 1792/ 1864]\n",
            "loss: 0.168373 [  232/ 1864]\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 76.3%, Avg loss: 0.014477 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.745265 [    0/ 1864]\n",
            "loss: 1.532652 [   64/ 1864]\n",
            "loss: 0.602508 [  128/ 1864]\n",
            "loss: 0.973454 [  192/ 1864]\n",
            "loss: 1.146196 [  256/ 1864]\n",
            "loss: 0.570937 [  320/ 1864]\n",
            "loss: 0.990394 [  384/ 1864]\n",
            "loss: 0.897045 [  448/ 1864]\n",
            "loss: 0.623664 [  512/ 1864]\n",
            "loss: 0.418006 [  576/ 1864]\n",
            "loss: 0.671945 [  640/ 1864]\n",
            "loss: 0.628874 [  704/ 1864]\n",
            "loss: 0.675845 [  768/ 1864]\n",
            "loss: 0.579096 [  832/ 1864]\n",
            "loss: 0.584042 [  896/ 1864]\n",
            "loss: 0.462538 [  960/ 1864]\n",
            "loss: 0.560078 [ 1024/ 1864]\n",
            "loss: 0.540692 [ 1088/ 1864]\n",
            "loss: 0.709460 [ 1152/ 1864]\n",
            "loss: 0.619231 [ 1216/ 1864]\n",
            "loss: 0.525101 [ 1280/ 1864]\n",
            "loss: 0.655107 [ 1344/ 1864]\n",
            "loss: 0.603799 [ 1408/ 1864]\n",
            "loss: 0.560579 [ 1472/ 1864]\n",
            "loss: 0.683231 [ 1536/ 1864]\n",
            "loss: 0.701105 [ 1600/ 1864]\n",
            "loss: 1.179694 [ 1664/ 1864]\n",
            "loss: 0.545663 [ 1728/ 1864]\n",
            "loss: 0.578082 [ 1792/ 1864]\n",
            "loss: 0.610154 [  232/ 1864]\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.010330 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.647630 [    0/ 1864]\n",
            "loss: 0.535086 [   64/ 1864]\n",
            "loss: 0.451285 [  128/ 1864]\n",
            "loss: 0.599165 [  192/ 1864]\n",
            "loss: 0.560669 [  256/ 1864]\n",
            "loss: 0.558075 [  320/ 1864]\n",
            "loss: 0.726420 [  384/ 1864]\n",
            "loss: 0.574853 [  448/ 1864]\n",
            "loss: 0.815040 [  512/ 1864]\n",
            "loss: 1.119558 [  576/ 1864]\n",
            "loss: 1.254109 [  640/ 1864]\n",
            "loss: 1.118444 [  704/ 1864]\n",
            "loss: 0.537086 [  768/ 1864]\n",
            "loss: 0.705034 [  832/ 1864]\n",
            "loss: 0.943875 [  896/ 1864]\n",
            "loss: 0.770011 [  960/ 1864]\n",
            "loss: 0.636073 [ 1024/ 1864]\n",
            "loss: 0.421013 [ 1088/ 1864]\n",
            "loss: 0.652435 [ 1152/ 1864]\n",
            "loss: 1.036016 [ 1216/ 1864]\n",
            "loss: 0.450317 [ 1280/ 1864]\n",
            "loss: 0.466464 [ 1344/ 1864]\n",
            "loss: 0.553511 [ 1408/ 1864]\n",
            "loss: 0.554515 [ 1472/ 1864]\n",
            "loss: 0.518793 [ 1536/ 1864]\n",
            "loss: 0.620396 [ 1600/ 1864]\n",
            "loss: 0.543498 [ 1664/ 1864]\n",
            "loss: 0.573196 [ 1728/ 1864]\n",
            "loss: 0.575331 [ 1792/ 1864]\n",
            "loss: 0.337669 [  232/ 1864]\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 76.3%, Avg loss: 0.009584 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.481908 [    0/ 1864]\n",
            "loss: 0.687811 [   64/ 1864]\n",
            "loss: 0.441831 [  128/ 1864]\n",
            "loss: 0.610631 [  192/ 1864]\n",
            "loss: 0.617542 [  256/ 1864]\n",
            "loss: 0.633918 [  320/ 1864]\n",
            "loss: 0.633263 [  384/ 1864]\n",
            "loss: 0.585864 [  448/ 1864]\n",
            "loss: 0.518912 [  512/ 1864]\n",
            "loss: 0.541641 [  576/ 1864]\n",
            "loss: 0.590489 [  640/ 1864]\n",
            "loss: 0.780715 [  704/ 1864]\n",
            "loss: 0.568407 [  768/ 1864]\n",
            "loss: 0.497330 [  832/ 1864]\n",
            "loss: 0.611944 [  896/ 1864]\n",
            "loss: 0.539582 [  960/ 1864]\n",
            "loss: 0.517532 [ 1024/ 1864]\n",
            "loss: 0.545631 [ 1088/ 1864]\n",
            "loss: 0.555318 [ 1152/ 1864]\n",
            "loss: 0.646907 [ 1216/ 1864]\n",
            "loss: 0.628208 [ 1280/ 1864]\n",
            "loss: 0.505938 [ 1344/ 1864]\n",
            "loss: 0.500453 [ 1408/ 1864]\n",
            "loss: 0.586972 [ 1472/ 1864]\n",
            "loss: 0.588545 [ 1536/ 1864]\n",
            "loss: 0.520142 [ 1600/ 1864]\n",
            "loss: 0.546003 [ 1664/ 1864]\n",
            "loss: 0.596570 [ 1728/ 1864]\n",
            "loss: 0.581539 [ 1792/ 1864]\n",
            "loss: 0.356010 [  232/ 1864]\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 76.3%, Avg loss: 0.009253 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.607239 [    0/ 1864]\n",
            "loss: 0.709911 [   64/ 1864]\n",
            "loss: 0.495879 [  128/ 1864]\n",
            "loss: 0.651764 [  192/ 1864]\n",
            "loss: 0.485404 [  256/ 1864]\n",
            "loss: 0.548553 [  320/ 1864]\n",
            "loss: 0.546956 [  384/ 1864]\n",
            "loss: 0.513234 [  448/ 1864]\n",
            "loss: 0.601527 [  512/ 1864]\n",
            "loss: 0.559852 [  576/ 1864]\n",
            "loss: 0.596528 [  640/ 1864]\n",
            "loss: 0.691746 [  704/ 1864]\n",
            "loss: 0.586876 [  768/ 1864]\n",
            "loss: 0.610735 [  832/ 1864]\n",
            "loss: 0.551683 [  896/ 1864]\n",
            "loss: 0.636794 [  960/ 1864]\n",
            "loss: 0.600553 [ 1024/ 1864]\n",
            "loss: 0.417916 [ 1088/ 1864]\n",
            "loss: 0.557404 [ 1152/ 1864]\n",
            "loss: 0.477015 [ 1216/ 1864]\n",
            "loss: 0.589096 [ 1280/ 1864]\n",
            "loss: 0.537769 [ 1344/ 1864]\n",
            "loss: 0.521079 [ 1408/ 1864]\n",
            "loss: 0.510145 [ 1472/ 1864]\n",
            "loss: 0.532771 [ 1536/ 1864]\n",
            "loss: 0.493204 [ 1600/ 1864]\n",
            "loss: 0.563944 [ 1664/ 1864]\n",
            "loss: 0.486494 [ 1728/ 1864]\n",
            "loss: 0.633452 [ 1792/ 1864]\n",
            "loss: 0.954609 [  232/ 1864]\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 76.3%, Avg loss: 0.009809 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.424582 [    0/ 1864]\n",
            "loss: 0.617107 [   64/ 1864]\n",
            "loss: 0.645292 [  128/ 1864]\n",
            "loss: 0.655091 [  192/ 1864]\n",
            "loss: 0.508571 [  256/ 1864]\n",
            "loss: 0.595329 [  320/ 1864]\n",
            "loss: 0.665532 [  384/ 1864]\n",
            "loss: 0.713535 [  448/ 1864]\n",
            "loss: 0.508865 [  512/ 1864]\n",
            "loss: 0.553379 [  576/ 1864]\n",
            "loss: 0.460379 [  640/ 1864]\n",
            "loss: 0.508232 [  704/ 1864]\n",
            "loss: 0.651981 [  768/ 1864]\n",
            "loss: 0.614503 [  832/ 1864]\n",
            "loss: 0.581714 [  896/ 1864]\n",
            "loss: 0.635934 [  960/ 1864]\n",
            "loss: 0.511573 [ 1024/ 1864]\n",
            "loss: 0.612018 [ 1088/ 1864]\n",
            "loss: 0.554366 [ 1152/ 1864]\n",
            "loss: 0.482416 [ 1216/ 1864]\n",
            "loss: 0.580141 [ 1280/ 1864]\n",
            "loss: 0.454358 [ 1344/ 1864]\n",
            "loss: 0.585237 [ 1408/ 1864]\n",
            "loss: 0.541367 [ 1472/ 1864]\n",
            "loss: 0.569660 [ 1536/ 1864]\n",
            "loss: 0.644025 [ 1600/ 1864]\n",
            "loss: 0.606937 [ 1664/ 1864]\n",
            "loss: 0.496306 [ 1728/ 1864]\n",
            "loss: 0.525058 [ 1792/ 1864]\n",
            "loss: 0.671769 [  232/ 1864]\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 76.3%, Avg loss: 0.009522 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.579895 [    0/ 1864]\n",
            "loss: 0.512701 [   64/ 1864]\n",
            "loss: 0.599440 [  128/ 1864]\n",
            "loss: 0.542564 [  192/ 1864]\n",
            "loss: 0.550136 [  256/ 1864]\n",
            "loss: 0.615922 [  320/ 1864]\n",
            "loss: 0.600818 [  384/ 1864]\n",
            "loss: 0.554198 [  448/ 1864]\n",
            "loss: 0.595759 [  512/ 1864]\n",
            "loss: 0.440210 [  576/ 1864]\n",
            "loss: 0.567372 [  640/ 1864]\n",
            "loss: 0.490178 [  704/ 1864]\n",
            "loss: 0.505561 [  768/ 1864]\n",
            "loss: 0.546917 [  832/ 1864]\n",
            "loss: 0.517073 [  896/ 1864]\n",
            "loss: 0.641499 [  960/ 1864]\n",
            "loss: 0.588916 [ 1024/ 1864]\n",
            "loss: 0.478327 [ 1088/ 1864]\n",
            "loss: 0.383145 [ 1152/ 1864]\n",
            "loss: 0.520877 [ 1216/ 1864]\n",
            "loss: 0.540412 [ 1280/ 1864]\n",
            "loss: 0.683020 [ 1344/ 1864]\n",
            "loss: 0.527458 [ 1408/ 1864]\n",
            "loss: 0.541878 [ 1472/ 1864]\n",
            "loss: 0.512346 [ 1536/ 1864]\n",
            "loss: 0.581303 [ 1600/ 1864]\n",
            "loss: 0.597439 [ 1664/ 1864]\n",
            "loss: 0.581002 [ 1728/ 1864]\n",
            "loss: 0.542674 [ 1792/ 1864]\n",
            "loss: 0.535130 [  232/ 1864]\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Error: \n",
            " Accuracy: 76.3%, Avg loss: 0.009980 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# label 0: Mild Diabetic Nonproliferative Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 0:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "      print(pred.argmax(1))\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW5Sj8z_fOeL"
      },
      "outputs": [],
      "source": [
        "torch.save(model2.state_dict(),  \"binary0model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8Fl_65Vd88B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae135c9-4a46-430c-bb0b-aed87501deb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.741083 [    0/ 1864]\n",
            "loss: 22.239538 [   64/ 1864]\n",
            "loss: 2.054401 [  128/ 1864]\n",
            "loss: 1.492692 [  192/ 1864]\n",
            "loss: 2.272995 [  256/ 1864]\n",
            "loss: 2.091600 [  320/ 1864]\n",
            "loss: 4.342196 [  384/ 1864]\n",
            "loss: 3.786101 [  448/ 1864]\n",
            "loss: 0.965240 [  512/ 1864]\n",
            "loss: 0.895054 [  576/ 1864]\n",
            "loss: 2.357228 [  640/ 1864]\n",
            "loss: 4.085869 [  704/ 1864]\n",
            "loss: 4.566681 [  768/ 1864]\n",
            "loss: 2.068973 [  832/ 1864]\n",
            "loss: 1.619917 [  896/ 1864]\n",
            "loss: 0.702635 [  960/ 1864]\n",
            "loss: 1.088595 [ 1024/ 1864]\n",
            "loss: 0.696505 [ 1088/ 1864]\n",
            "loss: 2.071493 [ 1152/ 1864]\n",
            "loss: 2.775524 [ 1216/ 1864]\n",
            "loss: 0.913383 [ 1280/ 1864]\n",
            "loss: 1.939711 [ 1344/ 1864]\n",
            "loss: 1.935739 [ 1408/ 1864]\n",
            "loss: 1.907807 [ 1472/ 1864]\n",
            "loss: 0.693241 [ 1536/ 1864]\n",
            "loss: 1.315539 [ 1600/ 1864]\n",
            "loss: 0.694579 [ 1664/ 1864]\n",
            "loss: 0.676846 [ 1728/ 1864]\n",
            "loss: 1.261357 [ 1792/ 1864]\n",
            "loss: 2.453486 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 47.8%, Avg loss: 0.021776 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.357193 [    0/ 1864]\n",
            "loss: 1.250357 [   64/ 1864]\n",
            "loss: 0.713018 [  128/ 1864]\n",
            "loss: 0.672187 [  192/ 1864]\n",
            "loss: 0.707306 [  256/ 1864]\n",
            "loss: 0.620988 [  320/ 1864]\n",
            "loss: 0.856005 [  384/ 1864]\n",
            "loss: 0.774101 [  448/ 1864]\n",
            "loss: 0.727493 [  512/ 1864]\n",
            "loss: 0.698985 [  576/ 1864]\n",
            "loss: 0.701723 [  640/ 1864]\n",
            "loss: 0.703099 [  704/ 1864]\n",
            "loss: 0.784409 [  768/ 1864]\n",
            "loss: 0.733600 [  832/ 1864]\n",
            "loss: 0.802716 [  896/ 1864]\n",
            "loss: 0.671009 [  960/ 1864]\n",
            "loss: 0.713428 [ 1024/ 1864]\n",
            "loss: 0.665812 [ 1088/ 1864]\n",
            "loss: 0.722928 [ 1152/ 1864]\n",
            "loss: 0.823259 [ 1216/ 1864]\n",
            "loss: 0.747872 [ 1280/ 1864]\n",
            "loss: 0.710512 [ 1344/ 1864]\n",
            "loss: 0.686827 [ 1408/ 1864]\n",
            "loss: 0.718604 [ 1472/ 1864]\n",
            "loss: 0.706316 [ 1536/ 1864]\n",
            "loss: 0.702474 [ 1600/ 1864]\n",
            "loss: 0.692086 [ 1664/ 1864]\n",
            "loss: 0.700482 [ 1728/ 1864]\n",
            "loss: 0.704032 [ 1792/ 1864]\n",
            "loss: 0.777393 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 57.8%, Avg loss: 0.012089 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.740454 [    0/ 1864]\n",
            "loss: 0.756050 [   64/ 1864]\n",
            "loss: 0.780739 [  128/ 1864]\n",
            "loss: 0.690155 [  192/ 1864]\n",
            "loss: 0.694261 [  256/ 1864]\n",
            "loss: 0.677527 [  320/ 1864]\n",
            "loss: 0.658864 [  384/ 1864]\n",
            "loss: 0.751118 [  448/ 1864]\n",
            "loss: 0.731990 [  512/ 1864]\n",
            "loss: 0.716801 [  576/ 1864]\n",
            "loss: 0.696100 [  640/ 1864]\n",
            "loss: 0.689910 [  704/ 1864]\n",
            "loss: 0.849598 [  768/ 1864]\n",
            "loss: 0.761566 [  832/ 1864]\n",
            "loss: 0.698767 [  896/ 1864]\n",
            "loss: 0.686013 [  960/ 1864]\n",
            "loss: 0.690972 [ 1024/ 1864]\n",
            "loss: 0.732628 [ 1088/ 1864]\n",
            "loss: 0.674978 [ 1152/ 1864]\n",
            "loss: 0.717665 [ 1216/ 1864]\n",
            "loss: 0.727654 [ 1280/ 1864]\n",
            "loss: 0.681210 [ 1344/ 1864]\n",
            "loss: 0.668947 [ 1408/ 1864]\n",
            "loss: 0.690362 [ 1472/ 1864]\n",
            "loss: 0.669926 [ 1536/ 1864]\n",
            "loss: 0.694319 [ 1600/ 1864]\n",
            "loss: 0.675629 [ 1664/ 1864]\n",
            "loss: 0.690895 [ 1728/ 1864]\n",
            "loss: 0.694508 [ 1792/ 1864]\n",
            "loss: 0.631558 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 57.8%, Avg loss: 0.011847 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.705178 [    0/ 1864]\n",
            "loss: 0.661426 [   64/ 1864]\n",
            "loss: 0.651394 [  128/ 1864]\n",
            "loss: 0.683282 [  192/ 1864]\n",
            "loss: 0.673315 [  256/ 1864]\n",
            "loss: 0.650411 [  320/ 1864]\n",
            "loss: 0.694784 [  384/ 1864]\n",
            "loss: 0.686913 [  448/ 1864]\n",
            "loss: 0.661526 [  512/ 1864]\n",
            "loss: 0.711021 [  576/ 1864]\n",
            "loss: 0.658128 [  640/ 1864]\n",
            "loss: 0.728285 [  704/ 1864]\n",
            "loss: 0.704471 [  768/ 1864]\n",
            "loss: 0.683729 [  832/ 1864]\n",
            "loss: 0.676133 [  896/ 1864]\n",
            "loss: 0.696841 [  960/ 1864]\n",
            "loss: 0.658770 [ 1024/ 1864]\n",
            "loss: 0.657897 [ 1088/ 1864]\n",
            "loss: 0.673416 [ 1152/ 1864]\n",
            "loss: 0.697437 [ 1216/ 1864]\n",
            "loss: 0.712309 [ 1280/ 1864]\n",
            "loss: 0.683055 [ 1344/ 1864]\n",
            "loss: 0.713091 [ 1408/ 1864]\n",
            "loss: 0.707460 [ 1472/ 1864]\n",
            "loss: 0.658162 [ 1536/ 1864]\n",
            "loss: 0.687031 [ 1600/ 1864]\n",
            "loss: 0.692233 [ 1664/ 1864]\n",
            "loss: 0.740326 [ 1728/ 1864]\n",
            "loss: 0.737931 [ 1792/ 1864]\n",
            "loss: 0.605279 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 58.2%, Avg loss: 0.011817 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.670677 [    0/ 1864]\n",
            "loss: 0.663296 [   64/ 1864]\n",
            "loss: 0.779173 [  128/ 1864]\n",
            "loss: 0.786407 [  192/ 1864]\n",
            "loss: 0.695132 [  256/ 1864]\n",
            "loss: 0.774458 [  320/ 1864]\n",
            "loss: 0.723342 [  384/ 1864]\n",
            "loss: 0.696194 [  448/ 1864]\n",
            "loss: 0.714997 [  512/ 1864]\n",
            "loss: 0.756133 [  576/ 1864]\n",
            "loss: 0.678709 [  640/ 1864]\n",
            "loss: 0.704222 [  704/ 1864]\n",
            "loss: 0.672553 [  768/ 1864]\n",
            "loss: 0.673844 [  832/ 1864]\n",
            "loss: 0.725473 [  896/ 1864]\n",
            "loss: 0.683988 [  960/ 1864]\n",
            "loss: 0.663385 [ 1024/ 1864]\n",
            "loss: 0.671861 [ 1088/ 1864]\n",
            "loss: 0.672340 [ 1152/ 1864]\n",
            "loss: 0.683031 [ 1216/ 1864]\n",
            "loss: 0.731811 [ 1280/ 1864]\n",
            "loss: 0.684764 [ 1344/ 1864]\n",
            "loss: 0.659656 [ 1408/ 1864]\n",
            "loss: 0.660695 [ 1472/ 1864]\n",
            "loss: 0.644632 [ 1536/ 1864]\n",
            "loss: 0.706233 [ 1600/ 1864]\n",
            "loss: 0.675696 [ 1664/ 1864]\n",
            "loss: 0.739582 [ 1728/ 1864]\n",
            "loss: 0.671207 [ 1792/ 1864]\n",
            "loss: 0.654698 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 57.3%, Avg loss: 0.011838 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.685864 [    0/ 1864]\n",
            "loss: 0.683418 [   64/ 1864]\n",
            "loss: 0.709648 [  128/ 1864]\n",
            "loss: 0.695749 [  192/ 1864]\n",
            "loss: 0.688967 [  256/ 1864]\n",
            "loss: 0.659231 [  320/ 1864]\n",
            "loss: 0.679259 [  384/ 1864]\n",
            "loss: 0.719939 [  448/ 1864]\n",
            "loss: 0.665387 [  512/ 1864]\n",
            "loss: 0.688731 [  576/ 1864]\n",
            "loss: 0.676945 [  640/ 1864]\n",
            "loss: 0.671028 [  704/ 1864]\n",
            "loss: 0.673505 [  768/ 1864]\n",
            "loss: 0.683793 [  832/ 1864]\n",
            "loss: 0.691937 [  896/ 1864]\n",
            "loss: 0.667898 [  960/ 1864]\n",
            "loss: 0.654217 [ 1024/ 1864]\n",
            "loss: 0.671915 [ 1088/ 1864]\n",
            "loss: 0.697452 [ 1152/ 1864]\n",
            "loss: 0.699615 [ 1216/ 1864]\n",
            "loss: 0.707819 [ 1280/ 1864]\n",
            "loss: 0.688900 [ 1344/ 1864]\n",
            "loss: 0.720615 [ 1408/ 1864]\n",
            "loss: 0.700880 [ 1472/ 1864]\n",
            "loss: 0.682338 [ 1536/ 1864]\n",
            "loss: 0.660147 [ 1600/ 1864]\n",
            "loss: 0.704861 [ 1664/ 1864]\n",
            "loss: 0.676705 [ 1728/ 1864]\n",
            "loss: 0.651830 [ 1792/ 1864]\n",
            "loss: 0.665790 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 57.8%, Avg loss: 0.011657 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n"
          ]
        }
      ],
      "source": [
        "# label 1: Moderate Diabetic Nonproliferative Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 1:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIhdpflLfIgI"
      },
      "outputs": [],
      "source": [
        "torch.save(model2.state_dict(),  \"binary1model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUewMTcscp0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f1554d-ae64-4d00-addb-cd38fc4c720a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.939327 [    0/ 1864]\n",
            "loss: 0.605872 [   64/ 1864]\n",
            "loss: 0.081005 [  128/ 1864]\n",
            "loss: 0.700599 [  192/ 1864]\n",
            "loss: 0.764113 [  256/ 1864]\n",
            "loss: 0.434182 [  320/ 1864]\n",
            "loss: 0.479547 [  384/ 1864]\n",
            "loss: 0.175173 [  448/ 1864]\n",
            "loss: 0.127433 [  512/ 1864]\n",
            "loss: 1.398418 [  576/ 1864]\n",
            "loss: 2.151008 [  640/ 1864]\n",
            "loss: 2.876752 [  704/ 1864]\n",
            "loss: 3.568269 [  768/ 1864]\n",
            "loss: 0.423833 [  832/ 1864]\n",
            "loss: 12.245138 [  896/ 1864]\n",
            "loss: 0.404482 [  960/ 1864]\n",
            "loss: 0.286029 [ 1024/ 1864]\n",
            "loss: 0.176831 [ 1088/ 1864]\n",
            "loss: 10.531519 [ 1152/ 1864]\n",
            "loss: 0.345751 [ 1216/ 1864]\n",
            "loss: 0.219497 [ 1280/ 1864]\n",
            "loss: 0.350072 [ 1344/ 1864]\n",
            "loss: 7.030770 [ 1408/ 1864]\n",
            "loss: 0.200916 [ 1472/ 1864]\n",
            "loss: 16.686541 [ 1536/ 1864]\n",
            "loss: 0.506054 [ 1600/ 1864]\n",
            "loss: 0.372632 [ 1664/ 1864]\n",
            "loss: 1.227910 [ 1728/ 1864]\n",
            "loss: 0.227851 [ 1792/ 1864]\n",
            "loss: 1.176767 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 93.1%, Avg loss: 0.012088 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.987263 [    0/ 1864]\n",
            "loss: 2.918924 [   64/ 1864]\n",
            "loss: 4.245890 [  128/ 1864]\n",
            "loss: 0.170247 [  192/ 1864]\n",
            "loss: 0.244591 [  256/ 1864]\n",
            "loss: 0.218739 [  320/ 1864]\n",
            "loss: 0.305300 [  384/ 1864]\n",
            "loss: 0.226567 [  448/ 1864]\n",
            "loss: 0.182609 [  512/ 1864]\n",
            "loss: 1.181281 [  576/ 1864]\n",
            "loss: 0.424076 [  640/ 1864]\n",
            "loss: 0.296516 [  704/ 1864]\n",
            "loss: 0.596820 [  768/ 1864]\n",
            "loss: 0.183564 [  832/ 1864]\n",
            "loss: 0.766258 [  896/ 1864]\n",
            "loss: 0.190900 [  960/ 1864]\n",
            "loss: 0.311341 [ 1024/ 1864]\n",
            "loss: 0.893007 [ 1088/ 1864]\n",
            "loss: 0.577913 [ 1152/ 1864]\n",
            "loss: 0.529826 [ 1216/ 1864]\n",
            "loss: 0.610858 [ 1280/ 1864]\n",
            "loss: 0.190967 [ 1344/ 1864]\n",
            "loss: 0.227471 [ 1408/ 1864]\n",
            "loss: 0.394474 [ 1472/ 1864]\n",
            "loss: 0.385083 [ 1536/ 1864]\n",
            "loss: 0.260544 [ 1600/ 1864]\n",
            "loss: 0.217647 [ 1664/ 1864]\n",
            "loss: 0.150980 [ 1728/ 1864]\n",
            "loss: 0.181115 [ 1792/ 1864]\n",
            "loss: 1.290057 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 93.1%, Avg loss: 0.005761 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.150535 [    0/ 1864]\n",
            "loss: 0.238401 [   64/ 1864]\n",
            "loss: 0.279426 [  128/ 1864]\n",
            "loss: 0.287717 [  192/ 1864]\n",
            "loss: 2.038462 [  256/ 1864]\n",
            "loss: 0.245746 [  320/ 1864]\n",
            "loss: 0.321329 [  384/ 1864]\n",
            "loss: 0.299520 [  448/ 1864]\n",
            "loss: 1.913339 [  512/ 1864]\n",
            "loss: 0.299573 [  576/ 1864]\n",
            "loss: 0.266559 [  640/ 1864]\n",
            "loss: 0.268541 [  704/ 1864]\n",
            "loss: 0.235922 [  768/ 1864]\n",
            "loss: 0.350839 [  832/ 1864]\n",
            "loss: 2.505090 [  896/ 1864]\n",
            "loss: 2.675611 [  960/ 1864]\n",
            "loss: 0.343263 [ 1024/ 1864]\n",
            "loss: 0.323456 [ 1088/ 1864]\n",
            "loss: 0.260959 [ 1152/ 1864]\n",
            "loss: 0.362668 [ 1216/ 1864]\n",
            "loss: 0.499710 [ 1280/ 1864]\n",
            "loss: 0.327691 [ 1344/ 1864]\n",
            "loss: 0.493959 [ 1408/ 1864]\n",
            "loss: 0.544346 [ 1472/ 1864]\n",
            "loss: 0.508202 [ 1536/ 1864]\n",
            "loss: 0.255154 [ 1600/ 1864]\n",
            "loss: 0.154249 [ 1664/ 1864]\n",
            "loss: 0.191498 [ 1728/ 1864]\n",
            "loss: 0.142733 [ 1792/ 1864]\n",
            "loss: 2.008698 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 93.1%, Avg loss: 0.004077 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.245064 [    0/ 1864]\n",
            "loss: 0.360048 [   64/ 1864]\n",
            "loss: 0.392376 [  128/ 1864]\n",
            "loss: 0.386434 [  192/ 1864]\n",
            "loss: 0.445705 [  256/ 1864]\n",
            "loss: 0.213219 [  320/ 1864]\n",
            "loss: 0.187342 [  384/ 1864]\n",
            "loss: 0.483982 [  448/ 1864]\n",
            "loss: 0.234733 [  512/ 1864]\n",
            "loss: 1.199774 [  576/ 1864]\n",
            "loss: 0.202492 [  640/ 1864]\n",
            "loss: 0.268420 [  704/ 1864]\n",
            "loss: 0.127670 [  768/ 1864]\n",
            "loss: 0.077036 [  832/ 1864]\n",
            "loss: 0.137924 [  896/ 1864]\n",
            "loss: 0.444762 [  960/ 1864]\n",
            "loss: 0.252886 [ 1024/ 1864]\n",
            "loss: 0.373529 [ 1088/ 1864]\n",
            "loss: 0.137906 [ 1152/ 1864]\n",
            "loss: 0.528458 [ 1216/ 1864]\n",
            "loss: 0.201910 [ 1280/ 1864]\n",
            "loss: 0.315204 [ 1344/ 1864]\n",
            "loss: 0.192981 [ 1408/ 1864]\n",
            "loss: 0.296684 [ 1472/ 1864]\n",
            "loss: 0.142800 [ 1536/ 1864]\n",
            "loss: 0.274338 [ 1600/ 1864]\n",
            "loss: 0.436398 [ 1664/ 1864]\n",
            "loss: 0.324178 [ 1728/ 1864]\n",
            "loss: 0.355520 [ 1792/ 1864]\n",
            "loss: 0.091097 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 92.7%, Avg loss: 0.004600 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.244794 [    0/ 1864]\n",
            "loss: 0.361742 [   64/ 1864]\n",
            "loss: 0.278657 [  128/ 1864]\n",
            "loss: 0.281028 [  192/ 1864]\n",
            "loss: 0.408585 [  256/ 1864]\n",
            "loss: 0.221281 [  320/ 1864]\n",
            "loss: 0.515598 [  384/ 1864]\n",
            "loss: 0.313521 [  448/ 1864]\n",
            "loss: 0.211135 [  512/ 1864]\n",
            "loss: 0.312990 [  576/ 1864]\n",
            "loss: 0.197862 [  640/ 1864]\n",
            "loss: 0.366272 [  704/ 1864]\n",
            "loss: 0.242780 [  768/ 1864]\n",
            "loss: 0.296157 [  832/ 1864]\n",
            "loss: 0.231256 [  896/ 1864]\n",
            "loss: 0.262717 [  960/ 1864]\n",
            "loss: 0.080278 [ 1024/ 1864]\n",
            "loss: 0.236865 [ 1088/ 1864]\n",
            "loss: 0.194166 [ 1152/ 1864]\n",
            "loss: 0.234601 [ 1216/ 1864]\n",
            "loss: 0.118472 [ 1280/ 1864]\n",
            "loss: 0.320473 [ 1344/ 1864]\n",
            "loss: 0.368575 [ 1408/ 1864]\n",
            "loss: 0.391804 [ 1472/ 1864]\n",
            "loss: 0.187637 [ 1536/ 1864]\n",
            "loss: 0.182327 [ 1600/ 1864]\n",
            "loss: 0.184722 [ 1664/ 1864]\n",
            "loss: 0.189515 [ 1728/ 1864]\n",
            "loss: 0.108268 [ 1792/ 1864]\n",
            "loss: 0.903531 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 93.1%, Avg loss: 0.004074 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.113948 [    0/ 1864]\n",
            "loss: 0.301124 [   64/ 1864]\n",
            "loss: 0.320671 [  128/ 1864]\n",
            "loss: 0.265948 [  192/ 1864]\n",
            "loss: 0.270783 [  256/ 1864]\n",
            "loss: 0.299771 [  320/ 1864]\n",
            "loss: 0.430698 [  384/ 1864]\n",
            "loss: 0.201314 [  448/ 1864]\n",
            "loss: 0.227323 [  512/ 1864]\n",
            "loss: 0.314747 [  576/ 1864]\n",
            "loss: 0.296346 [  640/ 1864]\n",
            "loss: 0.388685 [  704/ 1864]\n",
            "loss: 0.340913 [  768/ 1864]\n",
            "loss: 0.375832 [  832/ 1864]\n",
            "loss: 0.392119 [  896/ 1864]\n",
            "loss: 0.183908 [  960/ 1864]\n",
            "loss: 0.286144 [ 1024/ 1864]\n",
            "loss: 0.259529 [ 1088/ 1864]\n",
            "loss: 0.156609 [ 1152/ 1864]\n",
            "loss: 0.369740 [ 1216/ 1864]\n",
            "loss: 0.233945 [ 1280/ 1864]\n",
            "loss: 0.273384 [ 1344/ 1864]\n",
            "loss: 0.306549 [ 1408/ 1864]\n",
            "loss: 0.153528 [ 1472/ 1864]\n",
            "loss: 0.229868 [ 1536/ 1864]\n",
            "loss: 0.148580 [ 1600/ 1864]\n",
            "loss: 0.193924 [ 1664/ 1864]\n",
            "loss: 0.237095 [ 1728/ 1864]\n",
            "loss: 0.099667 [ 1792/ 1864]\n",
            "loss: 0.033166 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 93.1%, Avg loss: 0.005015 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.234574 [    0/ 1864]\n",
            "loss: 0.281405 [   64/ 1864]\n",
            "loss: 0.234273 [  128/ 1864]\n",
            "loss: 0.235028 [  192/ 1864]\n",
            "loss: 0.087002 [  256/ 1864]\n",
            "loss: 0.425336 [  320/ 1864]\n",
            "loss: 0.232253 [  384/ 1864]\n",
            "loss: 0.237335 [  448/ 1864]\n",
            "loss: 0.141365 [  512/ 1864]\n",
            "loss: 0.282185 [  576/ 1864]\n",
            "loss: 0.320500 [  640/ 1864]\n",
            "loss: 0.317308 [  704/ 1864]\n",
            "loss: 0.474763 [  768/ 1864]\n",
            "loss: 0.100140 [  832/ 1864]\n",
            "loss: 0.301796 [  896/ 1864]\n",
            "loss: 0.229059 [  960/ 1864]\n",
            "loss: 0.249339 [ 1024/ 1864]\n",
            "loss: 0.236330 [ 1088/ 1864]\n",
            "loss: 0.209552 [ 1152/ 1864]\n",
            "loss: 0.338026 [ 1216/ 1864]\n",
            "loss: 0.238987 [ 1280/ 1864]\n",
            "loss: 0.276660 [ 1344/ 1864]\n",
            "loss: 0.271636 [ 1408/ 1864]\n",
            "loss: 0.392346 [ 1472/ 1864]\n",
            "loss: 0.195343 [ 1536/ 1864]\n",
            "loss: 0.275136 [ 1600/ 1864]\n",
            "loss: 0.117100 [ 1664/ 1864]\n",
            "loss: 0.265986 [ 1728/ 1864]\n",
            "loss: 0.270087 [ 1792/ 1864]\n",
            "loss: 0.408136 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 93.1%, Avg loss: 0.004330 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# label 2: Severe Diabetic Nonproliferative Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 2:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw5FXRRCfF98"
      },
      "outputs": [],
      "source": [
        "torch.save(model2.state_dict(),  \"binary2model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLexeoXcLylg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d34ee1f-3e8e-4f2d-d09e-0cac54793a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.603056 [    0/ 1864]\n",
            "loss: 3.309623 [   64/ 1864]\n",
            "loss: 0.180033 [  128/ 1864]\n",
            "loss: 0.451704 [  192/ 1864]\n",
            "loss: 0.707533 [  256/ 1864]\n",
            "loss: 0.566629 [  320/ 1864]\n",
            "loss: 0.359512 [  384/ 1864]\n",
            "loss: 0.263396 [  448/ 1864]\n",
            "loss: 0.483336 [  512/ 1864]\n",
            "loss: 0.310663 [  576/ 1864]\n",
            "loss: 0.182248 [  640/ 1864]\n",
            "loss: 0.288021 [  704/ 1864]\n",
            "loss: 0.677274 [  768/ 1864]\n",
            "loss: 0.438520 [  832/ 1864]\n",
            "loss: 0.343908 [  896/ 1864]\n",
            "loss: 0.456282 [  960/ 1864]\n",
            "loss: 0.310718 [ 1024/ 1864]\n",
            "loss: 0.390299 [ 1088/ 1864]\n",
            "loss: 1.707536 [ 1152/ 1864]\n",
            "loss: 0.305196 [ 1216/ 1864]\n",
            "loss: 0.204378 [ 1280/ 1864]\n",
            "loss: 0.374779 [ 1344/ 1864]\n",
            "loss: 1.329537 [ 1408/ 1864]\n",
            "loss: 0.310541 [ 1472/ 1864]\n",
            "loss: 0.378168 [ 1536/ 1864]\n",
            "loss: 1.719177 [ 1600/ 1864]\n",
            "loss: 3.734402 [ 1664/ 1864]\n",
            "loss: 17.483799 [ 1728/ 1864]\n",
            "loss: 3.801965 [ 1792/ 1864]\n",
            "loss: 2.926520 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.016085 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.388518 [    0/ 1864]\n",
            "loss: 4.164637 [   64/ 1864]\n",
            "loss: 0.233344 [  128/ 1864]\n",
            "loss: 0.348755 [  192/ 1864]\n",
            "loss: 0.446330 [  256/ 1864]\n",
            "loss: 0.309470 [  320/ 1864]\n",
            "loss: 0.295061 [  384/ 1864]\n",
            "loss: 0.237184 [  448/ 1864]\n",
            "loss: 0.286517 [  512/ 1864]\n",
            "loss: 0.399668 [  576/ 1864]\n",
            "loss: 0.188969 [  640/ 1864]\n",
            "loss: 0.392306 [  704/ 1864]\n",
            "loss: 0.307541 [  768/ 1864]\n",
            "loss: 0.339053 [  832/ 1864]\n",
            "loss: 0.473246 [  896/ 1864]\n",
            "loss: 0.385117 [  960/ 1864]\n",
            "loss: 0.235867 [ 1024/ 1864]\n",
            "loss: 0.238444 [ 1088/ 1864]\n",
            "loss: 0.226785 [ 1152/ 1864]\n",
            "loss: 0.367812 [ 1216/ 1864]\n",
            "loss: 0.193990 [ 1280/ 1864]\n",
            "loss: 0.294788 [ 1344/ 1864]\n",
            "loss: 0.295708 [ 1408/ 1864]\n",
            "loss: 0.351676 [ 1472/ 1864]\n",
            "loss: 1.232310 [ 1536/ 1864]\n",
            "loss: 0.517703 [ 1600/ 1864]\n",
            "loss: 0.382818 [ 1664/ 1864]\n",
            "loss: 0.222512 [ 1728/ 1864]\n",
            "loss: 0.620775 [ 1792/ 1864]\n",
            "loss: 0.613241 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 85.3%, Avg loss: 0.035539 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.962944 [    0/ 1864]\n",
            "loss: 2.482077 [   64/ 1864]\n",
            "loss: 1.772263 [  128/ 1864]\n",
            "loss: 0.864561 [  192/ 1864]\n",
            "loss: 0.308175 [  256/ 1864]\n",
            "loss: 0.388153 [  320/ 1864]\n",
            "loss: 0.191563 [  384/ 1864]\n",
            "loss: 0.397837 [  448/ 1864]\n",
            "loss: 0.355293 [  512/ 1864]\n",
            "loss: 0.248697 [  576/ 1864]\n",
            "loss: 0.233596 [  640/ 1864]\n",
            "loss: 0.281545 [  704/ 1864]\n",
            "loss: 0.276553 [  768/ 1864]\n",
            "loss: 0.176495 [  832/ 1864]\n",
            "loss: 0.176032 [  896/ 1864]\n",
            "loss: 1.500367 [  960/ 1864]\n",
            "loss: 0.266497 [ 1024/ 1864]\n",
            "loss: 0.346777 [ 1088/ 1864]\n",
            "loss: 3.723130 [ 1152/ 1864]\n",
            "loss: 0.259817 [ 1216/ 1864]\n",
            "loss: 0.311138 [ 1280/ 1864]\n",
            "loss: 1.314953 [ 1344/ 1864]\n",
            "loss: 0.231037 [ 1408/ 1864]\n",
            "loss: 0.394790 [ 1472/ 1864]\n",
            "loss: 0.226649 [ 1536/ 1864]\n",
            "loss: 0.341752 [ 1600/ 1864]\n",
            "loss: 0.230550 [ 1664/ 1864]\n",
            "loss: 0.264921 [ 1728/ 1864]\n",
            "loss: 0.327015 [ 1792/ 1864]\n",
            "loss: 1.058796 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.005106 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.301441 [    0/ 1864]\n",
            "loss: 0.319406 [   64/ 1864]\n",
            "loss: 0.252728 [  128/ 1864]\n",
            "loss: 0.314886 [  192/ 1864]\n",
            "loss: 0.213360 [  256/ 1864]\n",
            "loss: 0.407136 [  320/ 1864]\n",
            "loss: 0.340433 [  384/ 1864]\n",
            "loss: 0.284326 [  448/ 1864]\n",
            "loss: 0.271826 [  512/ 1864]\n",
            "loss: 0.612734 [  576/ 1864]\n",
            "loss: 0.232763 [  640/ 1864]\n",
            "loss: 0.400116 [  704/ 1864]\n",
            "loss: 0.236149 [  768/ 1864]\n",
            "loss: 0.153746 [  832/ 1864]\n",
            "loss: 0.810058 [  896/ 1864]\n",
            "loss: 0.439173 [  960/ 1864]\n",
            "loss: 0.310422 [ 1024/ 1864]\n",
            "loss: 0.286991 [ 1088/ 1864]\n",
            "loss: 0.430520 [ 1152/ 1864]\n",
            "loss: 0.253039 [ 1216/ 1864]\n",
            "loss: 0.230069 [ 1280/ 1864]\n",
            "loss: 0.303683 [ 1344/ 1864]\n",
            "loss: 0.160462 [ 1408/ 1864]\n",
            "loss: 0.231088 [ 1472/ 1864]\n",
            "loss: 0.795912 [ 1536/ 1864]\n",
            "loss: 0.231323 [ 1600/ 1864]\n",
            "loss: 0.235074 [ 1664/ 1864]\n",
            "loss: 0.192548 [ 1728/ 1864]\n",
            "loss: 0.266999 [ 1792/ 1864]\n",
            "loss: 0.008839 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.005042 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.672752 [    0/ 1864]\n",
            "loss: 0.398005 [   64/ 1864]\n",
            "loss: 0.042425 [  128/ 1864]\n",
            "loss: 0.481415 [  192/ 1864]\n",
            "loss: 0.191708 [  256/ 1864]\n",
            "loss: 0.283912 [  320/ 1864]\n",
            "loss: 0.310145 [  384/ 1864]\n",
            "loss: 0.197661 [  448/ 1864]\n",
            "loss: 0.367160 [  512/ 1864]\n",
            "loss: 0.333156 [  576/ 1864]\n",
            "loss: 0.343912 [  640/ 1864]\n",
            "loss: 0.277547 [  704/ 1864]\n",
            "loss: 0.330697 [  768/ 1864]\n",
            "loss: 0.383710 [  832/ 1864]\n",
            "loss: 0.159597 [  896/ 1864]\n",
            "loss: 0.312810 [  960/ 1864]\n",
            "loss: 0.286184 [ 1024/ 1864]\n",
            "loss: 0.244315 [ 1088/ 1864]\n",
            "loss: 0.320852 [ 1152/ 1864]\n",
            "loss: 0.273570 [ 1216/ 1864]\n",
            "loss: 0.666593 [ 1280/ 1864]\n",
            "loss: 0.236702 [ 1344/ 1864]\n",
            "loss: 0.320055 [ 1408/ 1864]\n",
            "loss: 0.110101 [ 1472/ 1864]\n",
            "loss: 0.243160 [ 1536/ 1864]\n",
            "loss: 0.322227 [ 1600/ 1864]\n",
            "loss: 0.192340 [ 1664/ 1864]\n",
            "loss: 0.431735 [ 1728/ 1864]\n",
            "loss: 0.322906 [ 1792/ 1864]\n",
            "loss: 0.011632 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.004901 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.469568 [    0/ 1864]\n",
            "loss: 0.315072 [   64/ 1864]\n",
            "loss: 0.344105 [  128/ 1864]\n"
          ]
        }
      ],
      "source": [
        "# label 3: Hypertensive  Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 3:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAwfR61Ee_5A"
      },
      "outputs": [],
      "source": [
        "torch.save(model2.state_dict(),  \"binary3model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_g5Kt4-d_xR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79b7d957-a51d-4c85-d868-7bcd3fdda372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.653616 [    0/ 1864]\n",
            "loss: 0.000003 [   64/ 1864]\n",
            "loss: 1.666794 [  128/ 1864]\n",
            "loss: 0.000000 [  192/ 1864]\n",
            "loss: 0.000000 [  256/ 1864]\n",
            "loss: 0.000000 [  320/ 1864]\n",
            "loss: 0.584355 [  384/ 1864]\n",
            "loss: 0.692171 [  448/ 1864]\n",
            "loss: 0.830454 [  512/ 1864]\n",
            "loss: 0.000001 [  576/ 1864]\n",
            "loss: 0.227490 [  640/ 1864]\n",
            "loss: 0.152749 [  704/ 1864]\n",
            "loss: 0.097274 [  768/ 1864]\n",
            "loss: 0.405761 [  832/ 1864]\n",
            "loss: 0.031809 [  896/ 1864]\n",
            "loss: 0.099928 [  960/ 1864]\n",
            "loss: 0.344418 [ 1024/ 1864]\n",
            "loss: 0.182164 [ 1088/ 1864]\n",
            "loss: 0.017796 [ 1152/ 1864]\n",
            "loss: 0.306815 [ 1216/ 1864]\n",
            "loss: 0.001012 [ 1280/ 1864]\n",
            "loss: 0.006450 [ 1344/ 1864]\n",
            "loss: 0.078558 [ 1408/ 1864]\n",
            "loss: 0.006148 [ 1472/ 1864]\n",
            "loss: 0.081165 [ 1536/ 1864]\n",
            "loss: 0.166832 [ 1600/ 1864]\n",
            "loss: 5.408880 [ 1664/ 1864]\n",
            "loss: 0.009323 [ 1728/ 1864]\n",
            "loss: 0.013250 [ 1792/ 1864]\n",
            "loss: 0.000033 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.002555 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.013834 [    0/ 1864]\n",
            "loss: 0.173140 [   64/ 1864]\n",
            "loss: 2.074563 [  128/ 1864]\n",
            "loss: 0.002563 [  192/ 1864]\n",
            "loss: 0.116858 [  256/ 1864]\n",
            "loss: 0.002579 [  320/ 1864]\n",
            "loss: 0.170412 [  384/ 1864]\n",
            "loss: 0.200815 [  448/ 1864]\n",
            "loss: 0.078994 [  512/ 1864]\n",
            "loss: 0.007257 [  576/ 1864]\n",
            "loss: 0.051070 [  640/ 1864]\n",
            "loss: 0.286128 [  704/ 1864]\n",
            "loss: 0.078325 [  768/ 1864]\n",
            "loss: 0.078125 [  832/ 1864]\n",
            "loss: 2.169956 [  896/ 1864]\n",
            "loss: 0.033425 [  960/ 1864]\n",
            "loss: 0.074886 [ 1024/ 1864]\n",
            "loss: 0.074692 [ 1088/ 1864]\n",
            "loss: 0.554509 [ 1152/ 1864]\n",
            "loss: 0.082524 [ 1216/ 1864]\n",
            "loss: 0.129779 [ 1280/ 1864]\n",
            "loss: 0.095618 [ 1344/ 1864]\n",
            "loss: 0.078119 [ 1408/ 1864]\n",
            "loss: 0.037350 [ 1472/ 1864]\n",
            "loss: 0.132665 [ 1536/ 1864]\n",
            "loss: 0.134906 [ 1600/ 1864]\n",
            "loss: 0.017437 [ 1664/ 1864]\n",
            "loss: 0.144934 [ 1728/ 1864]\n",
            "loss: 0.009013 [ 1792/ 1864]\n",
            "loss: 0.003859 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.001119 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.077022 [    0/ 1864]\n",
            "loss: 0.217463 [   64/ 1864]\n",
            "loss: 0.322434 [  128/ 1864]\n",
            "loss: 0.180314 [  192/ 1864]\n",
            "loss: 0.086874 [  256/ 1864]\n",
            "loss: 0.046696 [  320/ 1864]\n",
            "loss: 0.106177 [  384/ 1864]\n",
            "loss: 0.104980 [  448/ 1864]\n",
            "loss: 0.075312 [  512/ 1864]\n",
            "loss: 0.944058 [  576/ 1864]\n",
            "loss: 0.012652 [  640/ 1864]\n",
            "loss: 0.009256 [  704/ 1864]\n",
            "loss: 0.155877 [  768/ 1864]\n",
            "loss: 0.037275 [  832/ 1864]\n",
            "loss: 0.018485 [  896/ 1864]\n",
            "loss: 0.077693 [  960/ 1864]\n",
            "loss: 0.075306 [ 1024/ 1864]\n",
            "loss: 0.077110 [ 1088/ 1864]\n",
            "loss: 0.165989 [ 1152/ 1864]\n",
            "loss: 0.008642 [ 1216/ 1864]\n",
            "loss: 0.006375 [ 1280/ 1864]\n",
            "loss: 0.079433 [ 1344/ 1864]\n",
            "loss: 0.007114 [ 1408/ 1864]\n",
            "loss: 0.160750 [ 1472/ 1864]\n",
            "loss: 0.003223 [ 1536/ 1864]\n",
            "loss: 0.084827 [ 1600/ 1864]\n",
            "loss: 0.079812 [ 1664/ 1864]\n",
            "loss: 0.014940 [ 1728/ 1864]\n",
            "loss: 0.006985 [ 1792/ 1864]\n",
            "loss: 0.000025 [  232/ 1864]\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.001915 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-8876a7d40868>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-8876a7d40868>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model2, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_changer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-44cc0d48f089>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-18a5e1353ba9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# label 4: Severe Diabetic Nonproliferative Retinopathy\n",
        "\n",
        "def label_changer2(y):\n",
        "  for i in range(len(y)):\n",
        "    y[i] = int(y[i])\n",
        "    if y[i] == 4:\n",
        "      y[i] = 1\n",
        "    else:\n",
        "      y[i] = 0\n",
        "    num = y[i]\n",
        "    y[i] = torch.tensor([[0, 0]], dtype=torch.float32)\n",
        "    y[i][0][num] = float(1)\n",
        "  y = torch.cat(y, dim=0)\n",
        "  # print(y)\n",
        "  return y\n",
        "model2 = NeuralNetwork2(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "batch_size = 64\n",
        "loss_fn =nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "def train_loop(dataloader, model2, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X = X.cpu()\n",
        "    y = list(y)\n",
        "    y = label_changer2(y)\n",
        "    y = y.cpu()\n",
        "    pred = model2(X)\n",
        "    loss = loss_fn(pred, y.argmax(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "def test_loop(dataloader, model2, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.cpu()\n",
        "      y = list(y)\n",
        "      y = label_changer2(y)\n",
        "      y = y.cpu()\n",
        "      pred = model2(X)\n",
        "      test_loss += loss_fn(pred, y.argmax(1)).item()\n",
        "      correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 7\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model2, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u-ijRihBmjF"
      },
      "outputs": [],
      "source": [
        "torch.save(model2.state_dict(),  \"binary4model\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}